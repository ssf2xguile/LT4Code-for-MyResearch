import logging
from typing import Dict, List, Any

from overrides import overrides

import torch
import numpy as np
from allennlp.data import Vocabulary, TextFieldTensors
from allennlp.models import Model
from allennlp.common import Params
from allennlp.modules import TextFieldEmbedder, FeedForward, Embedding
from allennlp.modules.text_field_embedders import BasicTextFieldEmbedder
from allennlp.modules.token_embedders import PretrainedTransformerEmbedder
from allennlp.modules.seq2seq_encoders import PytorchSeq2SeqWrapper, LstmSeq2SeqEncoder
from allennlp.modules.seq2vec_encoders import CnnEncoder, BagOfEmbeddingsEncoder, BertPooler, ClsPooler, CodeT5Pooler

from allennlp.nn import RegularizerApplicator, InitializerApplicator, Activation
from allennlp.nn.util import get_text_field_mask, get_final_encoder_states, weighted_sum
from allennlp.training.metrics import CategoricalAccuracy, FBetaMeasure, F1Measure, Metric, metric
from allennlp.training.util import get_batch_size

from torch import nn
from torch.nn import Dropout, PairwiseDistance, CosineSimilarity
import torch.nn.functional as F
from torch.autograd import Variable

from .custom_metric import ClassifPathMetric
from .custom_modules import pooling_with_mask

import warnings
import json
from copy import deepcopy
import pickle
warnings.filterwarnings("ignore")

logger = logging.getLogger(__name__)

import torch
import torch.nn as nn
from torch.autograd import Variable
import copy
import torch.nn.functional as F

def focal_loss(labels, logits, alpha, gamma):
    """Compute the focal loss between `logits` and the ground truth `labels`.
    Focal loss = -alpha_t * (1-pt)^gamma * log(pt)
    where pt is the probability of being classified to the true class.
    pt = p (if true class), otherwise pt = 1 - p. p = sigmoid(logit).
    Args:
      labels: A float tensor of size [batch, num_classes].
      logits: A float tensor of size [batch, num_classes].
      alpha: A float tensor of size [batch_size]
        specifying per-example weight for balanced cross entropy.
      gamma: A float scalar modulating loss from hard and easy examples.
    Returns:
      focal_loss: A float32 scalar representing normalized total loss.
    """
    BCLoss = F.binary_cross_entropy_with_logits(input = logits, target = labels,reduction = "none")

    if gamma == 0.0:
        modulator = 1.0
    else:
        modulator = torch.exp(-gamma * labels * logits - gamma * torch.log(1 +
            torch.exp(-1.0 * logits)))

    loss = modulator * BCLoss

    weighted_loss = alpha * loss
    focal_loss = torch.sum(weighted_loss)

    focal_loss /= torch.sum(labels)
    return focal_loss

# def CB_loss(labels, logits, samples_per_cls, no_of_classes, loss_type, beta, gamma):
#     ##Source: https://github.com/vandit15/Class-balanced-loss-pytorch/blob/master/class_balanced_loss.py
#     """Compute the Class Balanced Loss between `logits` and the ground truth `labels`.
#     Class Balanced Loss: ((1-beta)/(1-beta^n))*Loss(labels, logits)
#     where Loss is one of the standard losses used for Neural Networks.
#     Args:
#       labels: A int tensor of size [batch].
#       logits: A float tensor of size [batch, no_of_classes].
#       samples_per_cls: A python list of size [no_of_classes].
#       no_of_classes: total number of classes. int
#       loss_type: string. One of "sigmoid", "focal", "softmax".
#       beta: float. Hyperparameter for Class balanced loss.
#       gamma: float. Hyperparameter for Focal loss.
#     Returns:
#       cb_loss: A float tensor representing class balanced loss
#     """
#     effective_num = 1.0 - np.power(beta, samples_per_cls)
#     weights = (1.0 - beta) / np.array(effective_num)
#     weights = weights / np.sum(weights) * no_of_classes
#
#     labels_one_hot = F.one_hot(labels, no_of_classes).float()
#
#     weights = torch.tensor(weights).float()
#     weights = weights.unsqueeze(0)
#     weights = weights.to(device)
#     weights = weights.repeat(labels_one_hot.shape[0],1) * labels_one_hot
#     weights = weights.sum(1)
#     weights = weights.unsqueeze(1)
#     weights = weights.repeat(1,no_of_classes)
#
#     if loss_type == "focal":
#         cb_loss = focal_loss(labels_one_hot, logits, weights, gamma)
#     elif loss_type == "sigmoid":
#         cb_loss = F.binary_cross_entropy_with_logits(input = logits,target = labels_one_hot, weights = weights)
#     elif loss_type == "softmax":
#         pred = logits.softmax(dim = 1)
#         cb_loss = F.binary_cross_entropy(input = pred, target = labels_one_hot, weight = weights)
#     return cb_loss

def balanced_softmax_loss(labels, logits, sample_per_class, reduction, invalid_label_index):
    """Compute the Balanced Softmax Loss between `logits` and the ground truth `labels`.
    Args:
      labels: A int tensor of size [batch].
      logits: A float tensor of size [batch, no_of_classes].
      sample_per_class: A int tensor of size [no of classes].
      reduction: string. One of "none", "mean", "sum"
    Returns:
      loss: A float tensor. Balanced Softmax Loss.
    """
    sample_per_class = torch.tensor(sample_per_class)
    spc = sample_per_class.type_as(logits)
    spc = spc.unsqueeze(0).expand(logits.shape[0], -1)
    logits = logits + spc.log()
    loss = F.cross_entropy(input=logits, target=labels, reduction=reduction, ignore_index=invalid_label_index)
    return loss

@Model.register("model_baseline_codebert")
class ModelBaselineCodeBERT(Model):
    def __init__(self,
                 vocab: Vocabulary,
                 text_field_embedder: TextFieldEmbedder,
                 dropout: float = 0.1,
                 label_file: str = 'valid_cwes.json',
                 cwe_path_file: str = 'cwe_path.json',
                 invalid_label_index: int = -1,
                 depth: int = 0,
                 device: str = "cpu",
                 initializer: InitializerApplicator = InitializerApplicator(),
                 regularizer: RegularizerApplicator = None) -> None:
        super().__init__(vocab)
        self._device = torch.device(device)
        self._dropout = Dropout(dropout)

        label_freq_dict = pickle.load(open('./data/class_freq_file.pkl', 'rb'))
        label_freq_dict['CWE-441'] = 0

        self._labels = json.load(open(label_file, 'r'))




        self._invalid_label_index = invalid_label_index
        self._depth = depth


        for one_l in self._labels[self._depth]:
            if one_l not in label_freq_dict.keys():
                label_freq_dict[one_l] = 0

        self.samples_per_cls = torch.Tensor([label_freq_dict[one_l]  for one_l in self._labels[self._depth]])
        
        self._text_field_embedder = text_field_embedder
        embedding_dim = self._text_field_embedder.get_output_dim()

        # PTM = "microsoft/codebert-base"
        # self._pooler = BertPooler(PTM, requires_grad=True, dropout=dropout)
        PTM = "Salesforce/codet5-base"
        self._pooler = CodeT5Pooler(PTM, requires_grad=True, dropout=dropout)
        
        hidden_dim = 512
        self._projector = nn.Sequential(
            FeedForward(embedding_dim, 1, [hidden_dim], torch.nn.ReLU(), dropout),
            nn.Linear(hidden_dim, len(self._labels[self._depth])),
        )
        
        # only focus on custom metrics
        cwe_path = json.load(open(cwe_path_file, 'r'))
        self._custom_metrics = ClassifPathMetric(depth=depth, cwe_path=cwe_path)

        self._metrics = {
            "accuracy": CategoricalAccuracy(),
            "f1-score_weighted": FBetaMeasure(beta=1.0, average="weighted", labels=range(len(self._labels[self._depth]))),  # return float
        }

        self._loss = torch.nn.CrossEntropyLoss(weight=None, ignore_index=self._invalid_label_index, reduction="mean")
        initializer(self)

    def forward(self,
                diff: TextFieldTensors,
                label: torch.IntTensor = None,
                metadata: List[Dict[str, Any]] = None) -> Dict[str, Any]:

        output_dict = dict()
        if metadata:
            output_dict["meta"] = metadata

        diff_mask = get_text_field_mask(diff, num_wrapping_dims=1, padding_id=1)
        diff_hunk_mask = diff_mask.float().sum(dim=-1) > 0
       
        # sequence field
        diff_embedding = self._text_field_embedder(diff, num_wrapping_dims=1)
        shape = diff_embedding.shape
        # print("diff_embedding:", diff_embedding.size())
        # print("diff_embedding:", diff_embedding.view(-1, shape[-2], shape[-1]))
        #diff_embedding_pooled = self._pooler(diff_embedding.view(-1, shape[-2], shape[-1]))
        diff_embedding_pooled = self._pooler(diff_embedding)
        # print("diff_embedding_pooled:", diff_embedding_pooled.size())
        #diff_embedding_pooled = pooling_with_mask(input=diff_embedding_pooled.view(shape[0], shape[1], -1), mask=diff_hunk_mask)
        #print("diff_embedding_pooled:", diff_embedding_pooled.size())
        # print("self._text_field_embedder.get_output_dim():", self._text_field_embedder.get_output_dim())


        logits = self._projector(diff_embedding_pooled)
        # print("logits", logits.size(), "label", label.size())
        # print(label)

        loss = self._loss(logits, label)
        #print('\nce orignal loss:', loss)
        labels = label
        logits = logits

        LT_solution = 'fl'



        ## Baseline CE
        if LT_solution == 'ce':
            # print(LT_solution)
            loss_fct = nn.CrossEntropyLoss(ignore_index=self._invalid_label_index, reduction = "mean")
            loss = loss_fct(logits.view(-1, logits.size(-1)), labels.view(-1))
            # print('\nce loss2:', loss)
        elif LT_solution == 'fl':
            # print(LT_solution)
            ## LT Solution 1: Focal Loss
            input_tensor = logits.view(-1, logits.size(-1))
            target_tensor = labels.view(-1)
            log_prob = F.log_softmax(input_tensor, dim=-1)
            prob = torch.exp(log_prob)
            focal_loss_ = F.nll_loss(
                ((1 - prob) ** 2.0) * log_prob,
                target_tensor,
                reduction='mean',
                ignore_index = self._invalid_label_index
            )
            # print('focal_loss_:', focal_loss_)
            loss = focal_loss_
            # print('focal loss', focal_loss_)
        elif LT_solution == 'afl':
            # print(LT_solution)
            ## LT Solution 2: Anti-Focal Loss
            input_tensor = logits.view(-1, logits.size(-1))
            target_tensor = labels.view(-1)
            log_prob = F.log_softmax(input_tensor, dim=-1)
            prob = torch.exp(log_prob)
            focal_loss_ = F.nll_loss(
                ((1 + prob) ** 2.0) * log_prob,
                target_tensor,
                reduction='mean',
                ignore_index = self._invalid_label_index
            )
            print('anti-focal_loss_:', focal_loss_)
            loss = focal_loss_


        elif LT_solution == 'bs':
            print(LT_solution)
            ## LT Solution 3: Balanced Softmax Loss
            input_tensor = logits.view(-1, logits.size(-1))
            target_tensor = labels.view(-1)
            b_soft_loss = balanced_softmax_loss(target_tensor, input_tensor, self.samples_per_cls, 'mean', self._invalid_label_index)
            loss = b_soft_loss
            print('bs_loss_:', loss)





        output_dict['loss'] = loss

        probs = nn.functional.softmax(logits, dim=-1)
        output_dict["probs"] = probs

        predicts = torch.argmax(probs, dim=-1)
        output_dict["predicts"] = predicts
        self._custom_metrics(predictions=[self._labels[self._depth][idx] for idx in predicts.tolist()], metadata=metadata)
        
        for metric_name, metric_ in self._metrics.items():
            if metadata[0]["type"] != "test" or "f1-score" not in metric_name:
                # in test, it is possible to have new labels (index=-1), metric API for f1-score will fail
                metric_(predictions=probs, gold_labels=label)

        return output_dict

    def make_output_human_readable(self, output_dict: Dict[str, Any]) -> Dict[str, Any]:
        label_idx = output_dict["predicts"].tolist()
        probs = output_dict["probs"].tolist()
        out2file = list()
        for i, idx in enumerate(label_idx):
            meta = output_dict["meta"][i]["instance"]
            meta["predict"] = self._labels[self._depth][idx]
            meta["prob"] = probs[i]
            out2file.append(meta)
        return out2file

    def get_metrics(self, reset: bool = False) -> Dict[str, float]:
        metrics = {}
        
        metrics['accuracy'] = self._metrics['accuracy'].get_metric(reset)
        try:
            precision, recall, fscore = self._metrics['f1-score_weighted'].get_metric(reset).values()
            metrics['weighted_precision'] = precision
            metrics['weighted_recall'] = recall
            metrics['weighted_f1-score'] = fscore
        except Exception:
            pass
        
        custom_metrics = deepcopy(self._custom_metrics.get_metric(reset))

        metrics = dict(metrics, **custom_metrics)

        return metrics
