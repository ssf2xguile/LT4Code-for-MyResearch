import json
from collections import Counter
import numpy as np
import matplotlib as mpl
import matplotlib.pyplot as plt
import seaborn as sns
import warnings;

warnings.filterwarnings(action='once')
import pandas as pd
import json
import json, os, random
import pandas as pd
from collections import Counter
from tqdm import tqdm
import numpy as np
import numpy as np
import math
import seaborn as sns
from sklearn.metrics import confusion_matrix
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.metrics import accuracy_score, average_precision_score, precision_score, f1_score, recall_score
from sklearn.metrics import precision_recall_fscore_support, accuracy_score, matthews_corrcoef
from sklearn.metrics import accuracy_score
from sklearn.metrics import classification_report
from sklearn.metrics import matthews_corrcoef
from transformers import RobertaTokenizerFast

tokenizer = RobertaTokenizerFast.from_pretrained("microsoft/codebert-base")


###TreeVul Data
splits = ['train', 'test','validation']
seen_msgs= []
data_dir = '../all_data/vulnerability_data/dataset/'
all_data = []
for split in splits:
    file_name = data_dir + split +'_set.json'
    with open (file_name, 'r') as f:
        d_multi_sample = json.load(f)
        for d in d_multi_sample:
            if d['commit_id'] not in seen_msgs:
                all_data.append(d)
                seen_msgs.append(d['commit_id'])
treevul_ids = list(range(len(all_data)))
treevul_labels = [d['cwe_list']  for d in all_data]
treevul_inputs = [' '.join(d['REM_DIFF']) + ' '.join(d['ADD_DIFF'])  for d in all_data]
vocab = Counter(treevul_labels)
vocab_tokens = [i[0] for i in vocab.most_common(len(vocab))]
vocab_samples = [i[1] for i in vocab.most_common(len(vocab))]
total_smaple = sum(vocab_samples)
vocab_taking_up_ratios = [ float(l/total_smaple) for l in vocab_samples]
head_classes, tail_classes = [],[]
cumulative = 0
check_boundary=0
for i in range(len(vocab_taking_up_ratios)):
    check_boundary=cumulative
    cumulative += vocab_taking_up_ratios[i]
    if check_boundary<0.5 and cumulative>0.5:
        if (1-2*check_boundary) <= (2*cumulative-1):
            tail_classes.append(vocab_tokens[i])
        else:
            head_classes.append(vocab_tokens[i])
        continue
    if cumulative <= 0.5:
        head_classes.append(vocab_tokens[i])
    else:
        tail_classes.append(vocab_tokens[i])

head_classes  = [int(l.split('-')[1]) for l in head_classes]
tail_classes  = [int(l.split('-')[1]) for l in tail_classes]
print(len(head_classes), len(tail_classes))



result_path = '../generated_predictions/vulnerability_pred/Results/TreeVul/CE/out_treevul_result.json'
result_path2 = '../generated_predictions/vulnerability_pred/Results/CodeT5/CE/out_baseline_codet5_result.json'
result_path3 = '../generated_predictions/vulnerability_pred/Results/CodeBERT/CE/out_baseline_codebert_result.json'

def process_results_for_plot(result_path, vocab_tokens, labels=None):
    data = []
    with open(result_path, 'r+') as f:
        for d in f.readlines():
            d = json.loads(d)
            if len(d) > 1:
                data.extend(d) ## list of samples;
            else:
                data.extend(d)


    label_level0 = [d['path'][0] for d in data]
    label_level1 = [d['path'][1] for d in data]
    label_level2 = [d['path'][2] for d in data]

    pred_level0, pred_level1, pred_level2, score_level0, score_level1, score_level2 = [],[],[],[],[],[]
    for d in data:
        try:
            pred_level2.append(d['predict_2'][0])
            score_level2.append(d['prob_2'][0])
        except:
            pred_level2.append(d['predict'])
            score_level2.append(d['prob'][0]) ## wrong one

    label_level2 = [int(l.split('-')[1]) for l in label_level2]
    pred_level2 = [int(l.split('-')[1]) for l in pred_level2]
    mode = "weighted"
    if labels == None:
        labels = label_level2
    predicts = pred_level2

    def getmetric(labels, predicts, mode='weighted'):
        predicts_remove_unknown_labels = []
        for p in predicts:
            if p not in labels:
                predicts_remove_unknown_labels.append(-1)
            else:
                predicts_remove_unknown_labels.append(p)
        predicts = predicts_remove_unknown_labels

        precision, recall, f1, _ = precision_recall_fscore_support(labels, predicts, average=mode)
        acc = accuracy_score(labels, predicts)
        return acc
        # return f1

    test_vocab = Counter(labels)
    test_vocab_tokens = [i[0] for i in test_vocab.most_common(len(test_vocab))]
    test_vocab_samples = [i[1] for i in test_vocab.most_common(len(test_vocab))]
    test_total_smaple = sum(test_vocab_samples)
    test_vocab_taking_up_ratios = [float(l / test_total_smaple) for l in test_vocab_samples]
    test_vocab_taking_up_ratios_till_now = []
    for i in range(len(test_vocab_taking_up_ratios)):
        test_vocab_taking_up_ratios_till_now.append(sum(test_vocab_taking_up_ratios[0:i]))

    test_label2ratio = {}
    for j in range(len(test_vocab_tokens)):
        test_labl = test_vocab_tokens[j]
        test_ratio_till_now = test_vocab_taking_up_ratios_till_now[j]
        test_label2ratio[test_labl] = test_ratio_till_now


    sorted_pred, sorted_label = [],[]
    for class_ in test_vocab_tokens:
        for i in range(len(labels)):
            label_ = labels[i]
            if label_ == int(class_):
                sorted_pred.append(predicts[i])
                sorted_label.append(label_)
    indexs_of_pred = list(range(len(sorted_pred)))


    pos_list = [0.1 * i for i in range(1, 11)]
    pos_list[4] = pos_list[4] - 0.02
    pos_list[5] = pos_list[5] + 0.02

    threshold_list = [np.quantile(indexs_of_pred, pos) for pos in pos_list]
    metrics_by_ratio = []
    prior_test_thresh = min(indexs_of_pred)
    f1__ = getmetric(sorted_label, sorted_pred)
    for i in range(len(threshold_list)):
        test_thresh = threshold_list[i]
        test_by_thresh_preds, test_by_thresh_golds = [], []
        for j in range(len(indexs_of_pred)):
            # if indexs_of_pred[j] <= test_thresh and indexs_of_pred[j] > prior_test_thresh:
            if indexs_of_pred[j] < test_thresh and indexs_of_pred[j] >= prior_test_thresh:
                test_by_thresh_preds.append(sorted_pred[j])
                test_by_thresh_golds.append(sorted_label[j])

        if len(test_by_thresh_preds) > 0:
            f1__ = getmetric(test_by_thresh_golds, test_by_thresh_preds)
            metrics_by_ratio.append(f1__)
        else:
            metrics_by_ratio.append(f1__)
        prior_test_thresh = test_thresh
    return metrics_by_ratio, labels




def Head_Tail_Sets_Results(result_path, vocab_tokens, head_classes, tail_classes, labels=None):
    data = []
    with open(result_path, 'r+') as f:
        for d in f.readlines():
            d = json.loads(d)
            if len(d) > 1:
                data.extend(d) ## list of samples;
            else:
                data.extend(d)


    label_level0 = [d['path'][0] for d in data]
    label_level1 = [d['path'][1] for d in data]
    label_level2 = [d['path'][2] for d in data]

    pred_level0, pred_level1, pred_level2, score_level0, score_level1, score_level2 = [],[],[],[],[],[]
    for d in data:
        try:
            pred_level2.append(d['predict_2'][0])
            score_level2.append(d['prob_2'][0])
        except:
            pred_level2.append(d['predict'])
            score_level2.append(d['prob'][0]) ## wrong one

    label_level2 = [int(l.split('-')[1]) for l in label_level2]
    pred_level2 = [int(l.split('-')[1]) for l in pred_level2]
    mode = "weighted"
    if labels == None:
        labels = label_level2
    predicts = pred_level2

    def getmetric(predicts, labels, mode='weighted'):
        precision, recall, f1, _ = precision_recall_fscore_support(labels, predicts, average=mode)

        predicts_remove_unknown_labels = []
        for p in predicts:
            if p not in labels:
                predicts_remove_unknown_labels.append(-1)
            else:
                predicts_remove_unknown_labels.append(p)
        predicts = predicts_remove_unknown_labels

        acc = accuracy_score(labels, predicts)
        # print('f1:', f1)
        return acc
        # return f1

    predictions = predicts
    golds = labels
    head_preds, head_labels, tail_preds, tail_labels = [], [], [], []
    for i in range(len(golds)):
        if golds[i] in head_classes:
            head_preds.append(predictions[i])
            head_labels.append(golds[i])
        elif golds[i] in tail_classes:
            tail_preds.append(predictions[i])
            tail_labels.append(golds[i])
        else:
            tail_preds.append(predictions[i])
            tail_labels.append(golds[i])


    print('ALL', len(golds))
    print('EM:', getmetric(predictions, golds))
    print('HEAD', len(head_labels))
    print('EM:',getmetric(head_preds, head_labels))
    print('TAIL', len(tail_labels))
    print('EM:',getmetric(tail_preds, tail_labels))



metrics_by_classes, test_labs = process_results_for_plot(result_path, vocab_tokens)
#metrics_by_classes2, _ = process_results_for_plot(result_path2, vocab_tokens, test_labs)
#metrics_by_classes3, _ = process_results_for_plot(result_path3, vocab_tokens, test_labs)

# from pandas import Series
# from matplotlib.pyplot import MultipleLocator
# scale = 0.6
# ids = list(range(len(metrics_by_classes)))
# ids2 = list(range(len(metrics_by_classes2)))
# series = pd.DataFrame({'TreeVul':metrics_by_classes, 'CodeT5':metrics_by_classes2, 'CodeBERT': metrics_by_classes3, 'x': list(range(1,11,1))})
# rolling = series.rolling(window=1)
# rolling_mean = rolling.mean()
# print(rolling_mean.head(10))
# rolling_mean.plot(x='x', color=['orangered','lightgreen', 'violet'], kind='line',  style=['-','-.','--'], linewidth=2, alpha=1, figsize=(9.6*scale, 7.2*scale), fontsize=13)
# plt.xlabel('Sorted Groups ID', fontsize=14)
# # plt.ylabel('Accuracy', fontsize=14)
# plt.ylabel('Weighted F1', fontsize=14)
# plt.xticks(fontsize=14)
# plt.yticks(fontsize=14)
# plt.legend(fontsize=14)
# plt.tight_layout()
# plt.show()

Head_Tail_Sets_Results(result_path, vocab_tokens, head_classes, tail_classes)
Head_Tail_Sets_Results(result_path2, vocab_tokens, head_classes, tail_classes, test_labs)
Head_Tail_Sets_Results(result_path3, vocab_tokens, head_classes, tail_classes, test_labs)


